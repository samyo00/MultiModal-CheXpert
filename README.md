# ğŸ©º Multi-Modal AI for Chest X-Ray Diagnosis (CheXpert)

## ğŸ“Œ Project Overview
This project develops a **multimodal AI** for **chest X-ray classification** by integrating **image features (CNN/Vision Transformers)** with **radiology reports (BERT/BioBERT)**. Inspired by **generalized zero-shot learning (GZSL)**, we use **feature disentanglement** to separate visual and textual domain knowledge, improving **multi-label disease classification** on the **CheXpert dataset**.

## ğŸ”¥ Key Features
- **ğŸ”— Multi-Modal AI** â†’ Combines **chest X-ray images** with **radiology reports**.
- **ğŸ–¼ï¸ Vision Backbone** â†’ CNN, Vision Transformers (ViTs), Swin-Transformer.
- **ğŸ“œ Language Backbone** â†’ BERT, BioBERT, or ClinicalBERT for **radiology text embeddings**.
- **ğŸ†• Zero-Shot & Few-Shot Learning** â†’ Classifies unseen diseases without labeled samples.
- **ğŸ§  Feature Disentanglement** â†’ Learns **separate latent spaces** for images & text.
- **âš¡ Contrastive Learning** â†’ Uses **CLIP-like contrastive loss** for vision-text alignment.
- **ğŸ“Š Explainability** â†’ Visualizes model decisions with **GradCAM & Attention Maps**.

## ğŸ“‚ Dataset
We use the **[CheXpert Dataset](https://stanfordmlgroup.github.io/competitions/chexpert/)**, which contains **chest X-rays and corresponding radiology reports** for multi-label classification.

## ğŸ“¦ Installation
### 1ï¸âƒ£ Clone the repository:
```bash
git clone https://github.com/yourusername/MultiModal-CheXpert.git
cd MultiModal-CheXpert
